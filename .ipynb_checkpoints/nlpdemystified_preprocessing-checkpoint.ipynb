{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM8kLxUEVc3Z"
   },
   "source": [
    "# Natural Language Processing Demystified | Preprocessing\n",
    "https://nlpdemystified.org<br>\n",
    "https://github.com/futuremojo/nlp-demystified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btimL_w92Q3P"
   },
   "source": [
    "### spaCy upgrade and package installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7Ll-fUK2VZs"
   },
   "source": [
    "At the time this notebook was created, spaCy had newer releases but Colab was still using version 2.x by default. So the first step is to upgrade spaCy.\n",
    "<br><br>\n",
    "**IMPORTANT**<br>\n",
    "If you're running this in the cloud rather than using a local Jupyter server on your machine, then the notebook will **timeout** after a period of inactivity. If that happens and you don't reconnect in time, you will need to upgrade spaCy again and reinstall the requisite statistical packages.\n",
    "<br><br>\n",
    "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
    "https://research.google.com/colaboratory/local-runtimes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8vW9svTE289D"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfJKSJEU2U_s"
   },
   "source": [
    "After importing spaCy, the next thing we need to do is load a suitable statistical model for our project. spaCy offers a variety of models for different languages. These models help with tokenization, part-of-speech tagging, named entity recognition, and more.\n",
    "\n",
    "Here, we're loading the **en_core_web_sm** model which is the smallest English model spaCy offers and is a good starting point for NLP tasks.<br>\n",
    "https://spacy.io/models/en#en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v6TGQff2iu6"
   },
   "source": [
    "Since we upgraded spaCy, we'll need to download the statistical model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4uOyHDNb2i5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python não foi encontrado; executar sem argumentos para instalar do Microsoft Store ou desabilitar este atalho em Configurações > Aplicativos > Configurações avançadas do aplicativo > Aliases de execução do aplicativo.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mWDrpxDk2_r2"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\venv\\ilumpy\\lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\venv\\ilumpy\\lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7YCbWtG3LJO"
   },
   "source": [
    "**en_core_web_sm** is trained on OntoNotes 5 which is an annotated corpus comprising news, blogs, transcripts, etc. Put simply, this means a bunch of documents were labelled with information such as how each sentence should be parsed, whether a particular word is a noun or adjective or other part-of-speech, whether a word is a special entity like a person or a real-world organization, and other language-related labels. A statistical model was then generated from these labelled documents.<br>\n",
    "https://catalog.ldc.upenn.edu/LDC2013T19\n",
    "<br><br>\n",
    "You can learn more about the available spaCy models at these links:<br>\n",
    "https://spacy.io/models<br>\n",
    "https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvF_udvi3OTO"
   },
   "source": [
    "After loading the model, the _nlp_ variable now references a **Language** class instance which contains language-specific rules for various tasks (e.g. tokenization) and a processing pipeline.<br>\n",
    "https://spacy.io/api/language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "DAYGtQpT3UNN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unmnGRu8D-wa"
   },
   "source": [
    "# Tokenization\n",
    "\n",
    "Course module for this demo:\n",
    "https://www.nlpdemystified.org/course/tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLUcGm3IbQki"
   },
   "source": [
    "### Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13twUCp2i_p8"
   },
   "source": [
    "We pass whatever text we want to process to _nlp_, which returns a **Doc** container object containing the tokenized text and a number of annotations for each token. These annotations are discussed in follow-up videos. You can learn more about the **Doc** object here:<br>\n",
    "https://spacy.io/api/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "BIoEJZ-IkHQ4"
   },
   "outputs": [],
   "source": [
    "# Sample sentence.\n",
    "s = \"He didn't want to pay $20 for this book.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMWZK3ZSk9-f"
   },
   "source": [
    "We can iterate over this **Doc** object and view the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "8SzqhZuulAe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'did', \"n't\", 'want', 'to', 'pay', '$', '20', 'for', 'this', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ai1obkB93GdD"
   },
   "source": [
    "Note how\n",
    "- \"didn't\" is separated into \"did\"  and \"n't\".\n",
    "- the currency symbol and amount are separated.\n",
    "- the period at the end of the sentence is its own token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWH49gIh3hqN"
   },
   "source": [
    "The **Doc** object can be indexed and sliced like a regular list. The **Doc** object contains **Token** and **Span** objects, which offer different views into the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "MwLrxRsE3oKI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n"
     ]
    }
   ],
   "source": [
    "# We can view an individual token by indexing into the Doc object.\n",
    "print(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "bGapNHYQFYVa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# A Doc object is a container of other objects, namely Token and Span objects.\n",
    "print(type(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "EtL2IgIAGOd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He didn't\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "# Slicing a Doc object returns a Span object.\n",
    "print(doc[0:3])\n",
    "print(type(doc[0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "xybH4jjYGo73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 0), ('did', 1), (\"n't\", 2), ('want', 3), ('to', 4), ('pay', 5), ('$', 6), ('20', 7), ('for', 8), ('this', 9), ('book', 10), ('.', 11)]\n"
     ]
    }
   ],
   "source": [
    "# Access a token's index in a sentence.\n",
    "print([(t.text, t.i) for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TqE980F4Vrt"
   },
   "source": [
    "Spacy's tokenization is _non-destructive_, which means the original input can be reconstructed from the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "OjXb8mR_DK-1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He didn't want to pay $20 for this book.\n"
     ]
    }
   ],
   "source": [
    "# You can view the original input like so:\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73vuSX7MDK79"
   },
   "source": [
    "You can learn more about the **Token** and **Span** objects here:<br>\n",
    "https://spacy.io/api/token<br>\n",
    "https://spacy.io/api/span\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lume_1UP6ySQ"
   },
   "source": [
    "We can also tokenize multiple sentences and access each sentence individually using the **Doc** object's _sents_ property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "mPZ86x0hDK4m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Either the well was very deep, or she fell very slowly, for she\n",
      "had plenty of time as she went down to look about her and to wonder what\n",
      "was going to happen next.\n",
      "1 First, she tried to look down and make out what\n",
      "she was coming to, but it was too dark to see anything; then she looked at\n",
      "the sides of the well, and noticed that they were filled with cupboards and\n",
      "book-shelves; here and there she saw maps and pictures hung upon pegs.\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"Either the well was very deep, or she fell very slowly, for she\n",
    "had plenty of time as she went down to look about her and to wonder what\n",
    "was going to happen next. First, she tried to look down and make out what\n",
    "she was coming to, but it was too dark to see anything; then she looked at\n",
    "the sides of the well, and noticed that they were filled with cupboards and\n",
    "book-shelves; here and there she saw maps and pictures hung upon pegs.\"\"\"\n",
    "\n",
    "doc = nlp(s)\n",
    "\n",
    "# Look at individual sentences (there should be two 'Span' objects).\n",
    "#print([sent for sent in doc.sents])\n",
    "i = 0\n",
    "for sent in doc.sents:\n",
    "    print(i, sent)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvSfDUyK06Qg"
   },
   "source": [
    "### Tokenization Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "fyywcBrCHzSk"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE:\n",
    "# 1) Tokenize the following text\n",
    "# 2) Iterate through the tokens to check whether there's a currency symbol.\n",
    "# 3) If there is, and the currency label is followed by a number, print\n",
    "#    both the symbol and the number.\n",
    "#\n",
    "# Look through https://spacy.io/api/token#attributes on how to check whether\n",
    "# a token is a currency symbol or a number.\n",
    "#\n",
    "# Expected output: \"$20\".\n",
    "s = \"He didn't want to pay $20 for this book.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the value $20\n"
     ]
    }
   ],
   "source": [
    "match1 = '$'\n",
    "previous = False\n",
    "\n",
    "for token in doc:\n",
    "    #print(token, match1)\n",
    "    if str(token) == '$':\n",
    "        #print('Found', token)\n",
    "        previous = True\n",
    "        previous_text = token\n",
    "    if previous == True:\n",
    "        if token.is_digit:\n",
    "            print('Found the value', str(previous_text) + str(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "skajI-OZDK0t"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Learn how the spaCy tokenizer works and how to customize it:\n",
    "# https://spacy.io/usage/linguistic-features#tokenization\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "Ikbnyb8rDKv9"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Read through spaCy-101 and if you're interested, check out their course\n",
    "# on spaCy itself (link on the page).\n",
    "# https://spacy.io/usage/spacy-101\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/james/miniconda3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/james/miniconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/james/miniconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/james/miniconda3/lib/python3.12/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /home/james/miniconda3/lib/python3.12/site-packages (from nltk) (4.66.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "MMArLP91DKUW"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Look up how to tokenize the sentence below using NLTK. The imports\n",
    "# are done for you. Does the NLTK tokenizer handle \"N.Y.C.\" correctly?\n",
    "#\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "s = \"Let's go to N.Y.C. for the weekend.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMbm9tTakDdy"
   },
   "source": [
    "**NOTE**: Different tokenizers will give subtly different results based on the rules they use. Experiment with different tokenizers and use the one best suited for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "doc = tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', \"'s\", 'go', 'to', 'N.Y.C.', 'for', 'the', 'weekend', '.']\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y.C.\n",
      "for\n",
      "the\n",
      "weekend\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(s)\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUsfYCpVT4nI"
   },
   "source": [
    "# Basic Preprocessing\n",
    "## Case-Folding, Stop Word Removal, Stemming, and Lemmatization.\n",
    "\n",
    "Course module for this demo:\n",
    "https://www.nlpdemystified.org/course/basic-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgDDrCeI8f-4"
   },
   "source": [
    "spaCy performs all these preprocessing steps (except stemming) behind the scenes for you. Inline with its non-destructive policy, the tokens aren't modified directly. Rather, each **Token** object has a number of attributes which can help you get views of your document with these pre-processing steps applied. The attributes a **Token** has can be found here:<br>\n",
    "https://spacy.io/api/token#attributes\n",
    "<br><br>\n",
    "More information about spaCy's processing pipeline:<br>\n",
    "https://spacy.io/usage/processing-pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "jDEMR6En1j3H"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "s = \"He told Dr. Lovato that he was done with the tests and would post the results shortly.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwA1ct0obYlR"
   },
   "source": [
    "### Case-Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biBPWrVd9BrK"
   },
   "source": [
    "View your document with case-folding using the *lower_* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "1nt4RpzdgQQL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'told', 'dr.', 'lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'tests', 'and', 'would', 'post', 'the', 'results', 'shortly', '.']\n"
     ]
    }
   ],
   "source": [
    "print([t.lower_ for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HL46I4sH9OMq"
   },
   "source": [
    "You can also apply conditions when generating these views. For example, we can skip case-folding if a token is the start of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "IO0PQ8IFhOlZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[He, 'told', 'dr.', 'lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'tests', 'and', 'would', 'post', 'the', 'results', 'shortly', '.']\n"
     ]
    }
   ],
   "source": [
    "print([t.lower_ if not t.is_sent_start else t for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7pTz8XJbmaT"
   },
   "source": [
    "### Stop Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZLqqmHa9cRx"
   },
   "source": [
    "spaCy comes with a default stop word list. To view your document with stop words removed, you can use the *is_stop* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "9kvXbuDEhOxu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'toward', 'he', 'whereupon', '‘s', 'them', 'seemed', 'when', 'rather', 'whence', 'none', 'there', 'becomes', 'keep', 'us', 'top', 'serious', 'somehow', 'although', 'now', 'part', 'whither', 'various', 'anywhere', 'whose', 'against', 'just', 'across', 'or', 'per', 'towards', 'moreover', 'she', 'only', 'onto', 'any', 'several', 'his', '‘ll', 'seems', 'anything', 'enough', 'often', 're', 'something', '’re', 'here', 'whenever', 'whereby', 'via', 'afterwards', 'her', 'both', 'those', 'of', 'put', 'make', 'ca', 'ever', 'mostly', 'on', 'full', 'a', 'n’t', 'yourselves', 'most', 'take', '’ve', 'through', 'once', 'everyone', 'seem', 'beyond', 'because', 'yet', 'without', 'hence', 'what', 'else', 'see', 'nine', 'less', 'during', 'my', 'him', 'nevertheless', 'more', 'every', '‘ve', 'almost', 'am', 'least', 'always', 'bottom', 'in', 'off', 'fifteen', 'hereafter', '’d', 'anyway', 'same', 'himself', 'quite', 'everywhere', 'next', 'but', 'please', 'hundred', 'nobody', 'last', 'thereupon', 'whole', 'latterly', 'as', 'does', 'besides', 'already', 'eight', '’s', 'below', 'between', 'may', 'twelve', 'five', 'yours', 'whether', 'might', 'anyone', 'at', 'former', 'were', 'however', 'would', 'show', 'before', 'itself', 'are', 'amount', 'among', 'until', 'fifty', 'amongst', 'wherein', 'herein', 'thru', 'front', 'becoming', 'hereby', '’ll', 'whom', 'you', 'either', 'wherever', 'ourselves', 'did', \"'ve\", 'first', 'how', 'have', 'noone', 'sixty', \"n't\", 'to', 'meanwhile', 'their', 'alone', 'from', 'all', 'behind', \"'re\", \"'s\", 'other', 'we', 'done', 'herself', 'many', 'though', \"'ll\", 'together', 'this', 'three', 'eleven', 'some', 'upon', 'elsewhere', 'give', 'no', 'third', 'used', 'than', 'side', 'also', 'over', 'thence', 'never', 'beside', 'few', 'say', 'become', 'still', 'must', 'around', 'ours', 'nowhere', 'above', 'with', 'therefore', 'latter', 'further', 'it', 'along', 'is', 'became', 'again', 'thereafter', \"'d\", 'such', 'myself', 'others', 'should', 'really', 'n‘t', 'very', 'go', '‘re', 'name', 'two', 'into', 'under', 'the', 'not', 'has', 'forty', 'be', 'anyhow', 'who', 'much', 'move', 'whoever', 'doing', 'well', 'hereupon', 'up', 'by', 'these', 'mine', 'even', 'somewhere', 'formerly', 'get', 'throughout', 'sometimes', 'beforehand', 'me', 'perhaps', 'indeed', 'made', 'nothing', 'own', 'and', 'down', '’m', '‘d', 'while', 'seeming', 'thus', 'was', '‘m', 'each', 'that', 'will', 'an', 'six', 'its', 'back', 'empty', 'whereas', 'your', 'where', 'twenty', 'neither', 'due', 'which', 'therein', 'another', 'call', 'i', 'for', 'they', 'sometime', 'since', 'thereby', 'otherwise', 'why', 'within', 'had', 'then', 'using', 'ten', 'nor', 'everything', 'if', 'whereafter', 'do', 'except', 'whatever', 'too', \"'m\", 'about', 'one', 'after', 'four', 'unless', 'can', 'being', 'been', 'namely', 'someone', 'cannot', 'so', 'could', 'our', 'hers', 'regarding', 'themselves', 'out', 'yourself'}\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "# spaCy's default stop word list.\n",
    "print(nlp.Defaults.stop_words)\n",
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "oAS1xmgOhO5y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[told, Dr., Lovato, tests, post, results, shortly, .]\n"
     ]
    }
   ],
   "source": [
    "print([t for t in doc if not t.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPd1aiLrbqcK"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKidP32Y_qcE"
   },
   "source": [
    "It's similar with lemmatization. You can view your document with lemmatization applied through the *lemma_* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "fhdRleESkzTu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'he'),\n",
       " ('told', 'tell'),\n",
       " ('Dr.', 'Dr.'),\n",
       " ('Lovato', 'Lovato'),\n",
       " ('that', 'that'),\n",
       " ('he', 'he'),\n",
       " ('was', 'be'),\n",
       " ('done', 'do'),\n",
       " ('with', 'with'),\n",
       " ('the', 'the'),\n",
       " ('tests', 'test'),\n",
       " ('and', 'and'),\n",
       " ('would', 'would'),\n",
       " ('post', 'post'),\n",
       " ('the', 'the'),\n",
       " ('results', 'result'),\n",
       " ('shortly', 'shortly'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t.text, t.lemma_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuaQJPjEjADE"
   },
   "source": [
    "### Basic Preprocessing Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uNdkuJqCA2A"
   },
   "source": [
    "spaCy doesn't support stemming natively. But for completeness, we can stem using **NLTK**. Specifically, we can use the *Snowball stemmer* which is an improved version of the *Porter stemmer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "_HQzMurVB13l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'told', 'dr.', 'lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'test', 'and', 'would', 'post', 'the', 'result', 'short', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/james/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# EXERCISE: Find out how to intialize the SnowballStemmer, then tokenize\n",
    "# and stem the sentence below.\n",
    "#\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s = 'He told Dr. Lovato that he was done with the tests and would post the results shortly.'\n",
    "\n",
    "# Initialize the stemmer here.\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Tokenize, stem, and print the tokens.\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(s)\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "print(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "wOXJI061npqN"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Find out how to add and remove your own stop words in spaCy. Add the\n",
    "# word 'told' as a stop word, test that it works, then remove it from\n",
    "# the stop word list.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "RLcCYIy-lP1u"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Read up on how to add your own custom attributes to Token objects\n",
    "# and try adding one of your own.\n",
    "# https://spacy.io/usage/processing-pipelines#custom-components-attributes\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9HLYYUt1kOP"
   },
   "source": [
    "#Advanced Preprocessing\n",
    "\n",
    "## Part-of-Speech Tagging, Named Entity Recognition, and Parsing.\n",
    "\n",
    "Course module for this demo:\n",
    "https://www.nlpdemystified.org/course/advanced-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr5SqjHwSWpI"
   },
   "source": [
    "spaCy performs Part-of-Speech (POS) tagging, Named Entity Recognition (NER), and parsing as part of its default pipeline in the *nlp* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "shgWRMCq1kmy"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "s = \"John watched an old movie at the cinema.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwMQgciGb3or"
   },
   "source": [
    "### Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA9LDzULTW1_"
   },
   "source": [
    "POS tags can be accessed through the *pos_* attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "0-9YRcSZ1kqq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'PROPN'),\n",
       " ('watched', 'VERB'),\n",
       " ('an', 'DET'),\n",
       " ('old', 'ADJ'),\n",
       " ('movie', 'NOUN'),\n",
       " ('at', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('cinema', 'NOUN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t.text, t.pos_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UZcgwejnYm8"
   },
   "source": [
    "To get a description for a POS tag, we can use _spacy.explain_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "D9SXNvnmnW5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_WbFDZ-Tqu9"
   },
   "source": [
    "The POS tags above are called *course-grained* tags. You can also access *fine-grained* tags through the *tag_* attribute. Fine-grained tags provide more detailed information about a token such as its tense and, if a word is a pronoun, what specific type of pronoun it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "1Z5oDzNr1kt2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'NNP'),\n",
       " ('watched', 'VBD'),\n",
       " ('an', 'DT'),\n",
       " ('old', 'JJ'),\n",
       " ('movie', 'NN'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('cinema', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t.text, t.tag_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPOaN9yOUN-I"
   },
   "source": [
    "So **NNP** refers specifically to a _singular pronoun_, and **VBD** is a verb in *past tense*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "pnfLDxoG1kxf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun, proper singular\n",
      "verb, past tense\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('NNP'))\n",
    "print(spacy.explain('VBD'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jte6K6HJb750"
   },
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2J2BjPyqWFEf"
   },
   "source": [
    "There are multiple ways to access named entities. One way is through the *ent_type_* attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "dWjNrX6koNVj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Volkswagen', 'ORG'),\n",
       " ('is', ''),\n",
       " ('developing', ''),\n",
       " ('an', ''),\n",
       " ('electric', ''),\n",
       " ('sedan', ''),\n",
       " ('which', ''),\n",
       " ('could', ''),\n",
       " ('potentially', ''),\n",
       " ('come', ''),\n",
       " ('to', ''),\n",
       " ('America', 'GPE'),\n",
       " ('next', 'DATE'),\n",
       " ('fall', 'DATE'),\n",
       " ('.', '')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Volkswagen is developing an electric sedan which could potentially come to America next fall.\"\n",
    "doc = nlp(s)\n",
    "\n",
    "[(t.text, t.ent_type_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJL4wfS6Wp9N"
   },
   "source": [
    "You can view spaCy's named entities annotations here:<br>\n",
    "https://spacy.io/api/annotation#named-entities\n",
    "\n",
    "or use _spacy.explain_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "iu4OiPwDo9So"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7p8IcNGpBTP"
   },
   "source": [
    "You can also check if a token is an entity before printing it by checking whether the _ent_type_ (note the lack of trailing underscore) attribute is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "0aBng8zdvjly"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Volkswagen', 'ORG'), ('America', 'GPE'), ('next', 'DATE'), ('fall', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "print([(t.text, t.ent_type_) for t in doc if t.ent_type != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lNS65_2XJIY"
   },
   "source": [
    "Another way is through the _ents_ property of the **Doc** object. Here, we iterate through _ents_ and print the entity itself and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "kSCzxs02vjdL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Volkswagen', 'ORG'), ('America', 'GPE'), ('next fall', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHfZmta8XX9Y"
   },
   "source": [
    "Note how \"next fall\" is outputted above as a single span when you use _ents_.\n",
    "<br><br>\n",
    "You can also access the positions of entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "mSzwRD0MvjTN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Volkswagen', 'ORG', 0, 10), ('America', 'GPE', 75, 82), ('next fall', 'DATE', 83, 92)]\n"
     ]
    }
   ],
   "source": [
    "print([(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvvQ9_7FdEHT"
   },
   "source": [
    "spaCy is bundled with visualizers for both parsing and named entities.<br>\n",
    "https://spacy.io/usage/visualizers\n",
    "<br><br>\n",
    "Here, we visualize the entities in our sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "87eLywmVZCdw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Volkswagen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is developing an electric sedan which could potentially come to \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    America\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    next fall\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# We need to set the 'jupyter' variable to True in order to output\n",
    "# the visualization directly. Otherwise, you'll get raw HTML.\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkNmqelTwTLZ"
   },
   "source": [
    "For domain-specific corpora, an NER tagger may need to be further fine-tuned. Here, we may want _The Martian_ tagged as a \"FILM\" (assuming that's our goal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "0bcIaah29MME"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ridley Scott\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " directed The \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Martian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = \"Ridley Scott directed The Martian.\"\n",
    "doc = nlp(s)\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noGuG3JvcEfs"
   },
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppWrztdJeO3J"
   },
   "source": [
    "Let's first visualize a parse to make it easier to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "xrvfA1TEvjJT"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"15ea1d0f011747b29c5c35cea9afe00d-0\" class=\"displacy\" width=\"1450\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">She</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">enrolled</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">course</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">university.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-15ea1d0f011747b29c5c35cea9afe00d-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-15ea1d0f011747b29c5c35cea9afe00d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-15ea1d0f011747b29c5c35cea9afe00d-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-15ea1d0f011747b29c5c35cea9afe00d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390.0,266.5 L398.0,254.5 382.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-15ea1d0f011747b29c5c35cea9afe00d-0-2\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-15ea1d0f011747b29c5c35cea9afe00d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-15ea1d0f011747b29c5c35cea9afe00d-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-15ea1d0f011747b29c5c35cea9afe00d-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,266.5 L753.0,254.5 737.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-15ea1d0f011747b29c5c35cea9afe00d-0-4\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-15ea1d0f011747b29c5c35cea9afe00d-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,266.5 L933.0,254.5 917.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-15ea1d0f011747b29c5c35cea9afe00d-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-15ea1d0f011747b29c5c35cea9afe00d-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-15ea1d0f011747b29c5c35cea9afe00d-0-6\" stroke-width=\"2px\" d=\"M945,264.5 C945,89.5 1270.0,89.5 1270.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-15ea1d0f011747b29c5c35cea9afe00d-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1270.0,266.5 L1278.0,254.5 1262.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = \"She enrolled in the course at the university.\"\n",
    "doc = nlp(s)\n",
    "\n",
    "# Note the 'style' argument is assigned a 'dep' flag this time around.\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRN7_SQ-fO5H"
   },
   "source": [
    "The visualization above is for a dependency parse (spaCy doesn't come with a constituency parser). For each pair of depencencies, spaCy visualizes the child (pointed to), the head (pointed from), and their relationship (the label arc). You can view the dependency annotations here:<br>\n",
    "https://spacy.io/api/annotation#dependency-parsing\n",
    "\n",
    "You can also use *spacy.explain* to get information on a particular annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "wvz1bLTZfqmv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nominal subject'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('nsubj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCvHyqHggIpd"
   },
   "source": [
    "The dependency labels themselves can be accessed through the *dep_* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "iX_BgpMVoNaj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('She', 'nsubj'),\n",
       " ('enrolled', 'ROOT'),\n",
       " ('in', 'prep'),\n",
       " ('the', 'det'),\n",
       " ('course', 'pobj'),\n",
       " ('at', 'prep'),\n",
       " ('the', 'det'),\n",
       " ('university', 'pobj'),\n",
       " ('.', 'punct')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t.text, t.dep_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tt7zLq0ugR7O"
   },
   "source": [
    "Note how the word 'enrolled' is the _ROOT_.\n",
    "<br><br>\n",
    "But the labels above don't show how the words are related to each other (the arcs). To get a better idea, you can print the head of each dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "X15EOIq0oNfF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('She', 'nsubj', 'enrolled'),\n",
       " ('enrolled', 'ROOT', 'enrolled'),\n",
       " ('in', 'prep', 'enrolled'),\n",
       " ('the', 'det', 'course'),\n",
       " ('course', 'pobj', 'in'),\n",
       " ('at', 'prep', 'enrolled'),\n",
       " ('the', 'det', 'university'),\n",
       " ('university', 'pobj', 'at'),\n",
       " ('.', 'punct', 'enrolled')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(t.text, t.dep_, t.head.text) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFXPL37Rg2xm"
   },
   "source": [
    "### Using spaCy's Matcher to find patterns\n",
    "spaCy comes with a host of pattern-matching functionality. Beyond regex, spaCy can match on a variety of attributes such as POS tags, entity labels, lemmas, dependencies, entire phrases, and a lot more. You can learn more here:<br>\n",
    "https://spacy.io/usage/rule-based-matching<br>\n",
    "https://explosion.ai/demos/matcher\n",
    "<br><br>\n",
    "Here, we try to search for patterns that may be useful for a hospitality bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "6v4hVnYmJuaK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['book a hotel', 'book a hotel room']\n"
     ]
    }
   ],
   "source": [
    "# The general Matcher is one of multiple matcher objects\n",
    "# included with spaCy.\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# We initialize the Matcher with the spaCy vocab object, which contains\n",
    "# words along with their labels and entities.\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "s = \"I want to book a hotel room.\"\n",
    "doc = nlp(s)\n",
    "\n",
    "# Patterns are expressed as an ordered sequence. Here, we're looking\n",
    "# to match occurrences starting with a 'book' string followed by\n",
    "# a determiner (DET) POS tag, then a noun POS tag.\n",
    "# The OP key marks the match as optional in some way.\n",
    "\n",
    "# Here, the DET POS (marked with '?') will match 0 or 1 times, and\n",
    "# the NOUN POS (marked with '+') will match 1 or more times.\n",
    "# See this link for more information:\n",
    "# https://spacy.io/usage/rule-based-matching#quantifiers\n",
    "pattern = [\n",
    "  {'TEXT': 'book'},\n",
    "  {'POS': 'DET', 'OP': '?'},\n",
    "  {'POS': 'NOUN', 'OP': '+'},\n",
    "]\n",
    "\n",
    "# We give our pattern a label and pass it to the matcher.\n",
    "matcher.add('USER_INTENT', [pattern])\n",
    "\n",
    "# Run the matcher over the doc.\n",
    "matches = matcher(doc)\n",
    "\n",
    "# For each match, the matcher returns a tuple specifying a match id, start,\n",
    "# and end of the match.\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dygcIKF9plib"
   },
   "source": [
    "The code above demonstrates the Matcher but is brittle.\n",
    "- What if \"book\" is capitalized?\n",
    "- What if a user types \"reserve\" instead of \"book\"?\n",
    "- How can we match on \"hotel room\" as a compound noun?\n",
    "- What if a user types \"book a flight and hotel room\"?\n",
    "\n",
    "Can you think of how you would handle these cases?\n",
    "<br><br>\n",
    "We could come up more rules to match different patterns, or perhaps just search for keywords based on POS and entities (e.g. a country) and present the user with a bunch of possible intentions and let them choose one, or have a bunch of different interpretation functions submit answers and select the most likely one based on what was historically accepted most often. We can also ask clarifying questions to narrow things down.\n",
    "<br><br>\n",
    "For example, for the last sentence, you could have a function scan through the **Doc** object's *noun_chunks* (phrases that have a noun as their head) and isolate keywords there along with potential conjunctions (e.g. \"and\").<br>\n",
    "https://spacy.io/usage/linguistic-features#noun-chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "xctXGD5K5Gvr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase: I, root head: want\n",
      "phrase: a flight and hotel room, root head: book\n",
      "phrase: Berlin, root head: in\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I want to book a flight and hotel room in Berlin.\")\n",
    "for noun_phrase in doc.noun_chunks:\n",
    "  print(\"phrase: {}, root head: {}\".format(noun_phrase, noun_phrase.root.head))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMsHWX-9EvXU"
   },
   "source": [
    "Using pure rules is a good place to start or prototype (especially if the domain is narrow with a tight set of use cases) but as our requirements get more sophisticated, we'll need to blend in other approaches such as classical models or perhaps deep learning (at the very least, maybe tune existing neural networks). spaCy's models can be updated with more examples to fine-tune predictions.<br>\n",
    "https://spacy.io/usage/training<br>\n",
    "<br>\n",
    "We'll keep learning more approaches as the course progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knyuUv9cqsoY"
   },
   "source": [
    "### Talkin' like Yoda\n",
    "Languages like English are built around the _subject-verb-object_ pattern. But if you're familiar with Yoda from Star Wars, he famously speaks in an _object-subject-verb pattern_. Using the information in a dependency parse, we can turn basic English sentences into Yoda-speak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "L9AydbEIqsRQ"
   },
   "outputs": [],
   "source": [
    "def yodize(s: str):\n",
    "  doc = nlp(s)\n",
    "  for t in doc:\n",
    "    if t.dep_ == \"ROOT\":\n",
    "\n",
    "      # Assuming our sentence is of the form subject-verb-object, we take\n",
    "      # everything after the root (likely verb) and put it in front, and\n",
    "      # likewise take everything before the root, and put it after.\n",
    "      seq = [doc[t.i + 1: -1].text, doc[0: t.i].text, t.text + '.']\n",
    "      seq[0] = seq[0].capitalize()\n",
    "      print(' '.join(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "uIa8Cziwqqnf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To texas I will fly.\n"
     ]
    }
   ],
   "source": [
    "yodize(\"I will fly to Texas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofEnieaJZ8eX"
   },
   "source": [
    "This is ok for simple sentences but starts getting weird with longer, more convoluted sentences. What are some ways you would improve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V92TUxWioNtq"
   },
   "source": [
    "### Advanced Preprocessing Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "6Ltil7XSyzMe"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Learn how to extend spaCy's NER models. Specifically, how to add new\n",
    "# entity names and entity types.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "1P58pxYkoN0j"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: using doc.ents, identify and print the dates in this sentence.\n",
    "# Expected output: ['Feb 13th', 'Feb 24th']\n",
    "#\n",
    "s = \"We'll be in Osaka on Feb 13th and leave on Feb 24th.\"\n",
    "doc = nlp(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "OVFi0bxCoN4N"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Read about spaCy's PhraseMatcher\n",
    "# https://spacy.io/usage/rule-based-matching#phrasematcher\n",
    "#\n",
    "# Using the PhraseMatcher, find the start and end index of all occurrences\n",
    "# of 'Caesar Augustus' and 'Roman Empire' (case-insensitive).\n",
    "#\n",
    "# Expected output: [(0, 2), (15, 17)]\n",
    "#\n",
    "from spacy.matcher import PhraseMatcher\n",
    "s = \"Caesar Augustus was the founder of the Roman Principate (the first phase of the Roman Empire).\"\n",
    "doc = nlp(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nhN7p9G8taJ"
   },
   "source": [
    "# Additional Reading and Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM4G2KWa8wXO"
   },
   "source": [
    "Read through this page to learn more about spaCy's language processing pipeline including what's going on under the hood, how to create custom components, disable certain components (e.g. NER) when they're unneeded, optimization tips, and best practices:<br>\n",
    "https://spacy.io/usage/processing-pipelines\n",
    "<br><br>\n",
    "Take the free and succinct spaCy course (available in multiple languages):<br>\n",
    "https://course.spacy.io/\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "nlpdemystified-preprocessing.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ilumpy",
   "language": "python",
   "name": "ilumpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
